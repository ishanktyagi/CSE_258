{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"300\"\n",
       "            src=\"homework1.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x231da025ba8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"homework1.pdf\", width=600, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function for parsing JSONdata\n",
    "def parseData(fname):\n",
    "    for l in fname:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('beer_50000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and parsing JSONdata\n",
    "data  =list(parseData(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beer/ABV': 6.2,\n",
       " 'beer/beerId': '48213',\n",
       " 'beer/brewerId': '10325',\n",
       " 'beer/name': 'Red Moon',\n",
       " 'beer/style': 'English Strong Ale',\n",
       " 'review/appearance': 3.0,\n",
       " 'review/aroma': 2.5,\n",
       " 'review/overall': 3.0,\n",
       " 'review/palate': 3.0,\n",
       " 'review/taste': 3.0,\n",
       " 'review/text': 'Dark red color, light beige foam, average.\\tIn the smell malt and caramel, not really light.\\tAgain malt and caramel in the taste, not bad in the end.\\tMaybe a note of honey in teh back, and a light fruitiness.\\tAverage body.\\tIn the aftertaste a light bitterness, with the malt and red fruit.\\tNothing exceptional, but not bad, drinkable beer.',\n",
       " 'review/timeStruct': {'hour': 13,\n",
       "  'isdst': 0,\n",
       "  'mday': 1,\n",
       "  'min': 44,\n",
       "  'mon': 3,\n",
       "  'sec': 57,\n",
       "  'wday': 6,\n",
       "  'yday': 60,\n",
       "  'year': 2009},\n",
       " 'review/timeUnix': 1235915097,\n",
       " 'user/profileName': 'stcules'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tasks — Regression (week 1):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.No of Reviews for each style of beer in the dataset (`beer/style`) ,  \n",
    "#  avg value of `review/taste` for reviews from each style  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgReview(data):\n",
    "    totalRev = dict()\n",
    "    revFreq = dict()\n",
    "    avgRev = dict()\n",
    "    for dict_ in data:\n",
    "        if dict_['beer/style'] not in totalRev:\n",
    "            totalRev[dict_['beer/style']] = 0\n",
    "            revFreq[dict_['beer/style']]  = 0\n",
    "            \n",
    "        totalRev[dict_['beer/style']] += dict_['review/taste']\n",
    "        revFreq [dict_['beer/style']] += 1\n",
    "    for style in totalRev:\n",
    "        avgRev[style] = round(totalRev[style]/revFreq[style], 6)\n",
    "    return avgRev, revFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgRev ,revFreq = avgReview(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Altbier': 165,\n",
       " 'American Adjunct Lager': 242,\n",
       " 'American Amber / Red Ale': 665,\n",
       " 'American Amber / Red Lager': 42,\n",
       " 'American Barleywine': 825,\n",
       " 'American Black Ale': 138,\n",
       " 'American Blonde Ale': 357,\n",
       " 'American Brown Ale': 314,\n",
       " 'American Dark Wheat Ale': 14,\n",
       " 'American Double / Imperial IPA': 3886,\n",
       " 'American Double / Imperial Pilsner': 14,\n",
       " 'American Double / Imperial Stout': 5964,\n",
       " 'American IPA': 4113,\n",
       " 'American Malt Liquor': 90,\n",
       " 'American Pale Ale (APA)': 2288,\n",
       " 'American Pale Lager': 123,\n",
       " 'American Pale Wheat Ale': 154,\n",
       " 'American Porter': 2230,\n",
       " 'American Stout': 591,\n",
       " 'American Strong Ale': 166,\n",
       " 'American Wild Ale': 98,\n",
       " 'Baltic Porter': 514,\n",
       " 'Belgian Dark Ale': 175,\n",
       " 'Belgian IPA': 128,\n",
       " 'Belgian Pale Ale': 144,\n",
       " 'Belgian Strong Dark Ale': 146,\n",
       " 'Belgian Strong Pale Ale': 632,\n",
       " 'Berliner Weissbier': 10,\n",
       " 'BiÃ¨re de Garde': 7,\n",
       " 'Black & Tan': 122,\n",
       " 'Bock': 148,\n",
       " 'Braggot': 26,\n",
       " 'California Common / Steam Beer': 11,\n",
       " 'Chile Beer': 11,\n",
       " 'Cream Ale': 69,\n",
       " 'Czech Pilsener': 1501,\n",
       " 'Doppelbock': 873,\n",
       " 'Dortmunder / Export Lager': 31,\n",
       " 'Dubbel': 165,\n",
       " 'Dunkelweizen': 61,\n",
       " 'Eisbock': 8,\n",
       " 'English Barleywine': 133,\n",
       " 'English Bitter': 267,\n",
       " 'English Brown Ale': 495,\n",
       " 'English Dark Mild Ale': 21,\n",
       " 'English India Pale Ale (IPA)': 175,\n",
       " 'English Pale Ale': 1324,\n",
       " 'English Pale Mild Ale': 21,\n",
       " 'English Porter': 367,\n",
       " 'English Stout': 136,\n",
       " 'English Strong Ale': 164,\n",
       " 'Euro Dark Lager': 144,\n",
       " 'Euro Pale Lager': 701,\n",
       " 'Euro Strong Lager': 329,\n",
       " 'Extra Special / Strong Bitter (ESB)': 667,\n",
       " 'Flanders Oud Bruin': 13,\n",
       " 'Flanders Red Ale': 2,\n",
       " 'Foreign / Export Stout': 55,\n",
       " 'Fruit / Vegetable Beer': 1355,\n",
       " 'German Pilsener': 586,\n",
       " 'Hefeweizen': 618,\n",
       " 'Herbed / Spiced Beer': 73,\n",
       " 'Irish Dry Stout': 101,\n",
       " 'Irish Red Ale': 83,\n",
       " 'Keller Bier / Zwickel Bier': 23,\n",
       " 'Kristalweizen': 7,\n",
       " 'KÃ¶lsch': 94,\n",
       " 'Lambic - Fruit': 6,\n",
       " 'Lambic - Unblended': 10,\n",
       " 'Light Lager': 503,\n",
       " 'Low Alcohol Beer': 7,\n",
       " 'Maibock / Helles Bock': 225,\n",
       " 'Milk / Sweet Stout': 69,\n",
       " 'Munich Dunkel Lager': 141,\n",
       " 'Munich Helles Lager': 650,\n",
       " 'MÃ¤rzen / Oktoberfest': 557,\n",
       " 'Oatmeal Stout': 102,\n",
       " 'Old Ale': 1052,\n",
       " 'Pumpkin Ale': 560,\n",
       " 'Quadrupel (Quad)': 119,\n",
       " 'Rauchbier': 1938,\n",
       " 'Russian Imperial Stout': 2695,\n",
       " 'Rye Beer': 1798,\n",
       " 'Saison / Farmhouse Ale': 141,\n",
       " 'Schwarzbier': 53,\n",
       " 'Scotch Ale / Wee Heavy': 2776,\n",
       " 'Scottish Ale': 78,\n",
       " 'Scottish Gruit / Ancient Herbed Ale': 65,\n",
       " 'Smoked Beer': 61,\n",
       " 'Tripel': 257,\n",
       " 'Vienna Lager': 33,\n",
       " 'Weizenbock': 13,\n",
       " 'Wheatwine': 455,\n",
       " 'Winter Warmer': 259,\n",
       " 'Witbier': 162}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Altbier': 3.40303,\n",
       " 'American Adjunct Lager': 2.948347,\n",
       " 'American Amber / Red Ale': 3.513534,\n",
       " 'American Amber / Red Lager': 3.690476,\n",
       " 'American Barleywine': 4.064242,\n",
       " 'American Black Ale': 3.873188,\n",
       " 'American Blonde Ale': 3.254902,\n",
       " 'American Brown Ale': 3.743631,\n",
       " 'American Dark Wheat Ale': 3.678571,\n",
       " 'American Double / Imperial IPA': 4.033325,\n",
       " 'American Double / Imperial Pilsner': 3.821429,\n",
       " 'American Double / Imperial Stout': 4.479963,\n",
       " 'American IPA': 4.000851,\n",
       " 'American Malt Liquor': 2.255556,\n",
       " 'American Pale Ale (APA)': 3.649694,\n",
       " 'American Pale Lager': 3.215447,\n",
       " 'American Pale Wheat Ale': 3.334416,\n",
       " 'American Porter': 4.081839,\n",
       " 'American Stout': 3.819797,\n",
       " 'American Strong Ale': 3.569277,\n",
       " 'American Wild Ale': 4.188776,\n",
       " 'Baltic Porter': 4.213035,\n",
       " 'Belgian Dark Ale': 3.34,\n",
       " 'Belgian IPA': 3.949219,\n",
       " 'Belgian Pale Ale': 3.739583,\n",
       " 'Belgian Strong Dark Ale': 3.695205,\n",
       " 'Belgian Strong Pale Ale': 4.056171,\n",
       " 'Berliner Weissbier': 3.55,\n",
       " 'BiÃ¨re de Garde': 3.928571,\n",
       " 'Black & Tan': 3.942623,\n",
       " 'Bock': 3.189189,\n",
       " 'Braggot': 3.807692,\n",
       " 'California Common / Steam Beer': 3.318182,\n",
       " 'Chile Beer': 3.954545,\n",
       " 'Cream Ale': 3.028986,\n",
       " 'Czech Pilsener': 3.609594,\n",
       " 'Doppelbock': 3.982818,\n",
       " 'Dortmunder / Export Lager': 3.419355,\n",
       " 'Dubbel': 3.736364,\n",
       " 'Dunkelweizen': 3.491803,\n",
       " 'Eisbock': 3.75,\n",
       " 'English Barleywine': 4.360902,\n",
       " 'English Bitter': 3.537453,\n",
       " 'English Brown Ale': 3.728283,\n",
       " 'English Dark Mild Ale': 3.785714,\n",
       " 'English India Pale Ale (IPA)': 3.471429,\n",
       " 'English Pale Ale': 3.483761,\n",
       " 'English Pale Mild Ale': 3.595238,\n",
       " 'English Porter': 3.707084,\n",
       " 'English Stout': 3.599265,\n",
       " 'English Strong Ale': 3.756098,\n",
       " 'Euro Dark Lager': 3.704861,\n",
       " 'Euro Pale Lager': 2.96291,\n",
       " 'Euro Strong Lager': 2.848024,\n",
       " 'Extra Special / Strong Bitter (ESB)': 3.685157,\n",
       " 'Flanders Oud Bruin': 3.923077,\n",
       " 'Flanders Red Ale': 3.25,\n",
       " 'Foreign / Export Stout': 3.254545,\n",
       " 'Fruit / Vegetable Beer': 3.607749,\n",
       " 'German Pilsener': 3.667235,\n",
       " 'Hefeweizen': 3.635113,\n",
       " 'Herbed / Spiced Beer': 3.445205,\n",
       " 'Irish Dry Stout': 3.623762,\n",
       " 'Irish Red Ale': 2.981928,\n",
       " 'Keller Bier / Zwickel Bier': 3.869565,\n",
       " 'Kristalweizen': 2.785714,\n",
       " 'KÃ¶lsch': 3.696809,\n",
       " 'Lambic - Fruit': 3.75,\n",
       " 'Lambic - Unblended': 3.3,\n",
       " 'Light Lager': 2.39662,\n",
       " 'Low Alcohol Beer': 2.714286,\n",
       " 'Maibock / Helles Bock': 3.746667,\n",
       " 'Milk / Sweet Stout': 3.782609,\n",
       " 'Munich Dunkel Lager': 3.780142,\n",
       " 'Munich Helles Lager': 3.959231,\n",
       " 'MÃ¤rzen / Oktoberfest': 3.593357,\n",
       " 'Oatmeal Stout': 3.77451,\n",
       " 'Old Ale': 4.096008,\n",
       " 'Pumpkin Ale': 3.7875,\n",
       " 'Quadrupel (Quad)': 3.596639,\n",
       " 'Rauchbier': 4.067853,\n",
       " 'Russian Imperial Stout': 4.300371,\n",
       " 'Rye Beer': 4.213571,\n",
       " 'Saison / Farmhouse Ale': 3.702128,\n",
       " 'Schwarzbier': 3.622642,\n",
       " 'Scotch Ale / Wee Heavy': 4.083393,\n",
       " 'Scottish Ale': 3.762821,\n",
       " 'Scottish Gruit / Ancient Herbed Ale': 3.907692,\n",
       " 'Smoked Beer': 3.196721,\n",
       " 'Tripel': 3.784047,\n",
       " 'Vienna Lager': 3.530303,\n",
       " 'Weizenbock': 3.384615,\n",
       " 'Wheatwine': 4.186813,\n",
       " 'Winter Warmer': 3.621622,\n",
       " 'Witbier': 3.527778}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgRev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple predictor with a single binary feature indicating whether a beer is an `American IPA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFeature(data):\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in data:\n",
    "        if d['beer/style'] == 'American IPA':\n",
    "            X.append([1, 1])\n",
    "        else:\n",
    "            X.append([1, 0])\n",
    "        y.append(d['review/taste'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = genFeature(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Linear Regression using numpy OLS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, residuals, rank, s = np.linalg.lstsq(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta     : 3.9152047420838927 0.08564621828574305\n",
      "Residuals : [25650.80867309]\n"
     ]
    }
   ],
   "source": [
    "print(\"Theta     :\", theta[0], theta[1] )\n",
    "print(\"Residuals :\", residuals) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta(n_cols):\n",
    "    return np.random.randn(n_cols)\n",
    "\n",
    "def hypo_linReg(X, theta):\n",
    "    return np.dot(X, theta.T)\n",
    "\n",
    "def costFun_linReg(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = hypo_linReg(X, theta)\n",
    "    return  ( np.sum( (h - y) ** 2) ) / m \n",
    "\n",
    "def cal_gradient_linReg(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = hypo_linReg(X, theta)\n",
    "    return ( np.dot(X.T, ( h - y )) ) / m\n",
    "\n",
    "def perform_linReg(X, y, lr=0.01, num_itr=100000):\n",
    "    n_cols = X.shape[1]\n",
    "    theta = init_theta(n_cols)\n",
    "    costs = []\n",
    "    for i in range(num_itr):\n",
    "        gradient = cal_gradient_linReg(X, y, theta)\n",
    "        theta -= lr * gradient\n",
    "        if( i%100 == 0):\n",
    "            costs.append(costFun_linReg(X, y, theta))\n",
    "        if( i %10000 == 0):\n",
    "            print(costFun_linReg(X, y, theta))\n",
    "    return theta, costs\n",
    "\n",
    "def predict_linReg(X, theta):\n",
    "    y_pred = hypo_linReg(X, theta)\n",
    "    return y_pred\n",
    "\n",
    "def mse_linReg(y_true, y_pred):\n",
    "    m = len(y_true)\n",
    "    mse = np.sum( (y_true - y_pred) ** 2 ) / m\n",
    "    return mse\n",
    "\n",
    "def r2_linReg(y_true, y_pred):\n",
    "    y_mean = np.mean(y_true)\n",
    "    sse = np.sum( (y_true - y_pred) ** 2 )\n",
    "    tss = np.sum( (y_true - y_mean) ** 2 )\n",
    "    return (1 - sse/tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, costs = perform_linReg(X, y, lr=0.01, num_itr=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_linReg(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta    :  [3.91520474 0.08564622]\n",
      "MSE      :  0.5130161734617366\n",
      "R2_score :  0.0010782641230113743\n"
     ]
    }
   ],
   "source": [
    "print(\"Theta    : \", theta)\n",
    "print(\"MSE      : \", mse_linReg(y, y_pred))\n",
    "print(\"R2_score : \", r2_linReg(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title('Cost reduction over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Linear Regression using sckit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coeficients : [0.08564622]\n",
      "intercept   : 3.9152047420838145\n",
      "MSE         : 0.5130161734617366\n",
      "r2_score    : 0.0010782641230113743\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X[:, 1:], y)\n",
    "y_pred = reg.predict(X[:, 1:])\n",
    "\n",
    "print(\"coeficients :\",reg.coef_)\n",
    "print(\"intercept   :\",reg.intercept_)\n",
    "print(\"MSE         :\",MSE(y, y_pred ))\n",
    "print(\"r2_score    :\",r2_score(y, y_pred ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theta[0] represent the y-intercept of regression line\n",
    "\n",
    "theta[1] represent the slope of regression line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Splitting the data into train-test sets , Finding MSE on training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:int(len(X)/2),1:], X[int(len(X)/2):,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y[:int(len(X)/2)], y[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Multile Regression using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coeficients:  [0.05606027]\n",
      "intercept:  3.9043563922942206\n",
      "MSE on train set:  0.558107286558669\n",
      "MSE on test set:  0.46841005096664573\n",
      "r2_score on train set: 0.0004442931134949202\n",
      "r2_score on test set: 7.227980262203282e-05\n"
     ]
    }
   ],
   "source": [
    "# linear Regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "print(\"coeficients: \",reg.coef_)\n",
    "print(\"intercept: \",reg.intercept_)\n",
    "print(\"MSE on train set: \",MSE(y_train, reg.predict(X_train)) )\n",
    "print(\"MSE on test set: \",MSE(y_test, reg.predict(X_test)))\n",
    "print(\"r2_score on train set:\",r2_score(y_train, reg.predict(X_train)) )\n",
    "print(\"r2_score on test set:\",r2_score(y_test, reg.predict(X_test)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extending the above model , incorporating binary features for every style of beer with ≥ 50 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFeatures(data, revFreq):\n",
    "    beer_rev50 = []\n",
    "    for beer_style in revFreq.keys():\n",
    "        if revFreq[beer_style] > 50:\n",
    "            beer_rev50.append(beer_style)\n",
    "\n",
    "    #print( \"beer styles with reviews count > 50 are :\",len(beer_rev50))\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in data:\n",
    "        x = []\n",
    "        for beer in beer_rev50:\n",
    "            x.append( d['beer/style'] == beer )\n",
    "        X.append(x)\n",
    "        y.append(d['review/taste'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = genFeatures(data, revFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 74), (50000,))"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:int(len(X)/2),:], X[int(len(X)/2):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y[:int(len(X)/2)], y[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Regression using Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.466161174215785\n",
      "1.5490885894803492\n",
      "0.9430198663226093\n",
      "0.7273682913968028\n",
      "0.6307755984632287\n",
      "0.5809299166878924\n",
      "0.5523048541673914\n",
      "0.5344712248326355\n",
      "0.5226539191642804\n",
      "0.5144452282235812\n"
     ]
    }
   ],
   "source": [
    "theta, costs = perform_linReg(X_train, y_train, lr=0.01, num_itr=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta                 : [2.16067365 3.70496231 3.71880122 2.71213301 3.63421292 2.33431577\n",
      " 3.01349467 3.60097688 3.27417137 3.62300486 2.17533747 4.09087111\n",
      " 4.23561594 2.77352253 3.74666144 3.39813055 0.22558877 3.06494389\n",
      " 0.98468172 2.68658586 3.57704447 4.30245993 3.41188565 3.87390864\n",
      " 3.45461356 2.2588773  4.06320225 2.46185677 3.57320825 3.47096659\n",
      " 3.15159761 3.57563592 3.94278607 4.0922619  3.95347531 2.00077233\n",
      " 3.36157604 0.33766567 3.96041667 3.28607378 3.50844877 3.264106\n",
      " 3.14865736 3.87378629 3.49047595 3.38924058 3.75011194 3.1762208\n",
      " 3.63327154 3.59017125 3.92288    2.71761546 3.80994151 4.44843488\n",
      " 1.73576346 2.85952762 3.39750714 2.89522068 3.67866154 3.66430649\n",
      " 3.93741678 3.71970596 3.25472511 1.79456744 3.73249376 2.51111882\n",
      " 1.54122883 2.54495123 4.07524272 3.720022   3.91330349 3.33223932\n",
      " 1.6709058  2.37291665]\n",
      "MSE on train set      : 0.5085291066874872\n",
      "R2_score on train set : 0.0892375302218158\n",
      "MSE on test set       : 1.0439487115649964\n",
      "R2_score on test set  : -1.2285458072557636\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_linReg(X_train, theta)\n",
    "print(\"Theta                 :\", theta)\n",
    "print(\"MSE on train set      :\", mse_linReg(y_train, y_pred))\n",
    "print(\"R2_score on train set :\", r2_linReg(y_train, y_pred) )\n",
    "y_pred = predict_linReg(X_test, theta)\n",
    "print(\"MSE on test set       :\", mse_linReg(y_test, y_pred))\n",
    "print(\"R2_score on test set  :\", r2_linReg(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Multiple Linear Regression Using scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coeficients:  [ 0.71136364  0.09815076  0.12193999 -0.4664673   0.02739474  0.12395105\n",
      "  0.22651515 -0.0058248  -0.2937747   0.01699134 -0.50681818  0.58195733\n",
      "  0.76540404 -0.83277972  0.14466111  0.11433566  0.39318182  0.26818182\n",
      "  0.25681818 -0.92019846 -0.02450111  0.69564175 -0.1798951   0.26709486\n",
      " -0.10223103  0.01818182  0.45638407 -0.58598485  0.10049889 -0.13106061\n",
      " -0.45410882  0.38356643  0.33596789  0.48544372  0.3763751  -0.65227273\n",
      "  0.44318182 -0.35681818  0.35359848  0.60472028 -0.09003966 -0.22045455\n",
      " -0.17824675  0.26761104 -0.11634199  0.1527972   0.16385851 -0.41285266\n",
      "  0.03420746  0.19621212  0.33580477 -0.74318182  0.20312334  0.8416167\n",
      " -0.10681818 -0.73802386 -0.18884943 -0.63806818  0.13786267  0.05895722\n",
      "  0.3305986   0.12651515 -0.31931818 -1.00681818  0.20984848  0.65508658\n",
      " -0.20681818 -0.38622995  0.46842454  0.11320382  0.3449362  -0.27061129\n",
      " -0.62765152 -1.23390152]\n",
      "intercept:  3.606818181818188\n",
      "MSE on training set:  0.3678402770901444\n",
      "MSE on testing set:  0.4336695104199656\n",
      "r2_score on train set: 0.3412075831238651\n",
      "r2_score on test set: 0.07423385988737574\n"
     ]
    }
   ],
   "source": [
    "# linear Regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "print(\"coeficients          : \",reg.coef_)\n",
    "print(\"intercept            : \",reg.intercept_)\n",
    "\n",
    "y_pred = reg.predict(X_train)\n",
    "print(\"MSE on train set     : \",MSE(y_train, reg.predict(X_train)) )\n",
    "print(\"r2_score on train set:\",r2_score(y_train, y_pred) )\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"MSE on test set      : \",MSE(y_test, reg.predict(X_test)) )\n",
    "print(\"r2_score on test set :\",r2_score(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks — Classification (week 2):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. A predictor that estimates whether a beer is an `American IPA` using two features:\n",
    "[`beer/ABV`,`review/taste`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ABV_taste(datum):\n",
    "    return [1, datum['beer/ABV'], datum['review/taste']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature_ABV_taste(d) for d in data]\n",
    "y = [d['beer/style'] == 'American IPA' for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:int(len(X)/2)], X[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y[:int(len(y)/2)], y[int(len(y)/2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(C=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy score :  0.9226\n",
      "confusion matrix :\n",
      " [[22594   246]\n",
      " [ 1689   471]]\n",
      "classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.93      0.99      0.96     22840\n",
      "       True       0.66      0.22      0.33      2160\n",
      "\n",
      "avg / total       0.91      0.92      0.90     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, y_pred )\n",
    "print(\"Train set accuracy score : \", train_acc )\n",
    "print(\"confusion matrix :\\n\", confusion_matrix(y_train, y_pred))\n",
    "print(\"classification report :\\n\",classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy score : 0.85632\n",
      "confusion matrix : \n",
      " [[21364  1683]\n",
      " [ 1909    44]]\n",
      "classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.92      0.93      0.92     23047\n",
      "       True       0.03      0.02      0.02      1953\n",
      "\n",
      "avg / total       0.85      0.86      0.85     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy score :\", test_acc )\n",
    "print(\"confusion matrix : \\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"classification report :\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Engineering using common words in `review/text` of   `American IPA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "textRevAmericanIPA = []\n",
    "for d in data:\n",
    "    if d['beer/style'] == 'American IPA':\n",
    "        textRevAmericanIPA.append(d['review/text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = []\n",
    "for text in textRevAmericanIPA:\n",
    "    tokens += tokenizer.tokenize(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "all_words = [w for w in tokens if not w in stop_words and len(w) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDist = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hops', 4686),\n",
       " ('head', 3942),\n",
       " ('The', 3872),\n",
       " ('beer', 3804),\n",
       " ('IPA', 3663),\n",
       " ('hop', 3343),\n",
       " ('malt', 3117),\n",
       " ('nice', 2959),\n",
       " ('citrus', 2865),\n",
       " ('good', 2613),\n",
       " ('one', 2040),\n",
       " ('orange', 1916),\n",
       " ('like', 1916),\n",
       " ('white', 1876),\n",
       " ('carbonation', 1825),\n",
       " ('color', 1799),\n",
       " ('glass', 1799),\n",
       " ('taste', 1772),\n",
       " ('This', 1760),\n",
       " ('grapefruit', 1754),\n",
       " ('well', 1745),\n",
       " ('bitterness', 1734),\n",
       " ('pine', 1700),\n",
       " ('bitter', 1695),\n",
       " ('bit', 1690),\n",
       " ('flavor', 1684),\n",
       " ('lacing', 1660),\n",
       " ('light', 1660),\n",
       " ('finish', 1543),\n",
       " ('aroma', 1541),\n",
       " ('sweet', 1508),\n",
       " ('little', 1448),\n",
       " ('amber', 1339),\n",
       " ('caramel', 1319),\n",
       " ('body', 1292),\n",
       " ('Very', 1235),\n",
       " ('medium', 1179),\n",
       " ('bottle', 1151),\n",
       " ('Pours', 1141),\n",
       " ('floral', 1132),\n",
       " ('hoppy', 1102),\n",
       " ('great', 1048),\n",
       " ('nose', 1044),\n",
       " ('really', 1036),\n",
       " ('balanced', 1029),\n",
       " ('notes', 1014),\n",
       " ('smell', 951),\n",
       " ('dry', 926),\n",
       " ('fresh', 915),\n",
       " ('would', 909)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqDist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feat = ['citrus', 'IPA', 'bitter', 'grapefruit','caramel']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customFeatures(datum):\n",
    "    return [1, \n",
    "            datum['beer/ABV'],\n",
    "           'citrus' in datum['review/text'],\n",
    "           'IPA' in datum['review/text'],\n",
    "           'bitter' in datum['review/text'],\n",
    "           'grapefruit' in datum['review/text'],\n",
    "           'caramel' in datum['review/text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [customFeatures(d) for d in data]\n",
    "y = [d['beer/style'] == 'American IPA' for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:int(len(X)/2)], X[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y[:int(len(X)/2)], y[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = svm.SVC(C=1000)\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy score :  0.96248\n",
      "confusion matrix :\n",
      " [[22353   487]\n",
      " [  451  1709]]\n",
      "classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      0.98      0.98     22840\n",
      "       True       0.78      0.79      0.78      2160\n",
      "\n",
      "avg / total       0.96      0.96      0.96     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf2.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, y_pred )\n",
    "\n",
    "print(\"Train set accuracy score : \", train_acc )\n",
    "print(\"confusion matrix :\\n\", confusion_matrix(y_train, y_pred))\n",
    "print(\"classification report :\\n\",classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy score : 0.9262\n",
      "confusion matrix : \n",
      " [[21729  1318]\n",
      " [  527  1426]]\n",
      "classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      0.94      0.96     23047\n",
      "       True       0.52      0.73      0.61      1953\n",
      "\n",
      "avg / total       0.94      0.93      0.93     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf2.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy score :\", test_acc )\n",
    "print(\"confusion matrix : \\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"classification report :\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Effect  of  regularization  constant C  on SVM  performance \n",
    "  C\n",
    "∈ [0.1,10,1000,100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C :  0.1 \t train set accuracy :  0.95192 \ttest set accuracy :  0.9448 \n",
      "\n",
      "C :  10 \t train set accuracy :  0.96176 \ttest set accuracy :  0.93616 \n",
      "\n",
      "C :  1000 \t train set accuracy :  0.96248 \ttest set accuracy :  0.9262 \n",
      "\n",
      "C :  100000 \t train set accuracy :  0.9644 \ttest set accuracy :  0.92496 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in [0.1, 10, 1000, 100000 ]:\n",
    "    clf3 = svm.SVC(C=c)\n",
    "    clf3.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf3.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_pred)\n",
    "    \n",
    "    y_pred = clf3.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"C : \", c,'\\t', \"train set accuracy : \", train_acc,\"\\ttest set accuracy : \", test_acc,\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature_ABV_taste(d) for d in data]\n",
    "y = [d['beer/style'] == 'American IPA' for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:int(len(X)/2)], X[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y[:int(len(X)/2)], y[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta(X):\n",
    "    n_coeff = X.shape[1]\n",
    "    return np.random.randn(n_coeff)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def hypo_logReg(X, theta):\n",
    "    z = np.dot(X, theta)\n",
    "    return sigmoid(z)\n",
    "\n",
    "def costFun_logReg(X, y, theta):\n",
    "    m = X.shape[0]\n",
    "    h = hypo_logReg(X, theta)\n",
    "    \n",
    "    return (-1/m)*( np.sum( y * np.log(h) + (1 - y) * np.log (1 - h) ) )\n",
    "\n",
    "def cal_gradient_logReg(X, y, theta):\n",
    "    h = hypo_logReg(X, theta)\n",
    "    return np.dot( X.T, (h - y) ) / len(y)\n",
    "\n",
    "def perform_logReg(X, y, lr=0.01, num_itr=100000):\n",
    "    theta = init_theta(X)\n",
    "    costs = []\n",
    "    for i in range(num_itr):\n",
    "        gradient = cal_gradient_logReg(X, y, theta)\n",
    "        theta -= lr * gradient\n",
    "        if( i % 100 == 0):\n",
    "            costs.append(costFun_logReg(X, y, theta))\n",
    "    return theta, costs\n",
    "\n",
    "def predict_logReg(X, theta):\n",
    "    h = hypo_logReg(X, theta)\n",
    "    y_pred = (h >= 0.5).astype(np.int)\n",
    "    return y_pred\n",
    "\n",
    "def accuracy_logReg(y_true, y_pred):\n",
    "    correct_pred =[ true == pred for true, pred in zip(y_true, y_pred)] \n",
    "    accuracy =  np.sum(correct_pred) * 1.0 / len(correct_pred)\n",
    "    return accuracy\n",
    "\n",
    "def classificationReport(y_true, y_pred):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, costs = perform_logReg(X_train, y_train, lr=0.01, num_itr=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"theta             : {}\".format(theta) )\n",
    "\n",
    "y_pred = predict_logReg(X_train, theta)\n",
    "print(\"train set accuracy: \", accuracy_logReg(y_train, y_pred) )\n",
    "\n",
    "y_pred = predict_logReg(X_test, theta)\n",
    "print(\"test set accuracy : \", accuracy_logReg(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title('Cost reduction over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"theta              : \", clf.intercept_, clf.coef_)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "print(\"train set accuracy :\", accuracy_score(y_train, y_pred))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"test set accuracy  : \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
