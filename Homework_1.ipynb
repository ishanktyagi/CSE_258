{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"300\"\n",
       "            src=\"homework1.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x231da025ba8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"homework1.pdf\", width=600, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function for parsing JSONdata\n",
    "def parseData(fname):\n",
    "    for l in fname:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('beer_50000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and parsing JSONdata\n",
    "data  =list(parseData(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beer/ABV': 6.2,\n",
       " 'beer/beerId': '48213',\n",
       " 'beer/brewerId': '10325',\n",
       " 'beer/name': 'Red Moon',\n",
       " 'beer/style': 'English Strong Ale',\n",
       " 'review/appearance': 3.0,\n",
       " 'review/aroma': 2.5,\n",
       " 'review/overall': 3.0,\n",
       " 'review/palate': 3.0,\n",
       " 'review/taste': 3.0,\n",
       " 'review/text': 'Dark red color, light beige foam, average.\\tIn the smell malt and caramel, not really light.\\tAgain malt and caramel in the taste, not bad in the end.\\tMaybe a note of honey in teh back, and a light fruitiness.\\tAverage body.\\tIn the aftertaste a light bitterness, with the malt and red fruit.\\tNothing exceptional, but not bad, drinkable beer.',\n",
       " 'review/timeStruct': {'hour': 13,\n",
       "  'isdst': 0,\n",
       "  'mday': 1,\n",
       "  'min': 44,\n",
       "  'mon': 3,\n",
       "  'sec': 57,\n",
       "  'wday': 6,\n",
       "  'yday': 60,\n",
       "  'year': 2009},\n",
       " 'review/timeUnix': 1235915097,\n",
       " 'user/profileName': 'stcules'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tasks — Regression (week 1):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.No of Reviews for each style of beer in the dataset (`beer/style`) ,  \n",
    "#  avg value of `review/taste` for reviews from each style  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgReview(data):\n",
    "    totalRev = dict()\n",
    "    revFreq = dict()\n",
    "    avgRev = dict()\n",
    "    for dict_ in data:\n",
    "        if dict_['beer/style'] not in totalRev:\n",
    "            totalRev[dict_['beer/style']] = 0\n",
    "            revFreq[dict_['beer/style']]  = 0\n",
    "            \n",
    "        totalRev[dict_['beer/style']] += dict_['review/taste']\n",
    "        revFreq [dict_['beer/style']] += 1\n",
    "    for style in totalRev:\n",
    "        avgRev[style] = round(totalRev[style]/revFreq[style], 6)\n",
    "    return avgRev, revFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgRev ,revFreq = avgReview(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Altbier': 165,\n",
       " 'American Adjunct Lager': 242,\n",
       " 'American Amber / Red Ale': 665,\n",
       " 'American Amber / Red Lager': 42,\n",
       " 'American Barleywine': 825,\n",
       " 'American Black Ale': 138,\n",
       " 'American Blonde Ale': 357,\n",
       " 'American Brown Ale': 314,\n",
       " 'American Dark Wheat Ale': 14,\n",
       " 'American Double / Imperial IPA': 3886,\n",
       " 'American Double / Imperial Pilsner': 14,\n",
       " 'American Double / Imperial Stout': 5964,\n",
       " 'American IPA': 4113,\n",
       " 'American Malt Liquor': 90,\n",
       " 'American Pale Ale (APA)': 2288,\n",
       " 'American Pale Lager': 123,\n",
       " 'American Pale Wheat Ale': 154,\n",
       " 'American Porter': 2230,\n",
       " 'American Stout': 591,\n",
       " 'American Strong Ale': 166,\n",
       " 'American Wild Ale': 98,\n",
       " 'Baltic Porter': 514,\n",
       " 'Belgian Dark Ale': 175,\n",
       " 'Belgian IPA': 128,\n",
       " 'Belgian Pale Ale': 144,\n",
       " 'Belgian Strong Dark Ale': 146,\n",
       " 'Belgian Strong Pale Ale': 632,\n",
       " 'Berliner Weissbier': 10,\n",
       " 'BiÃ¨re de Garde': 7,\n",
       " 'Black & Tan': 122,\n",
       " 'Bock': 148,\n",
       " 'Braggot': 26,\n",
       " 'California Common / Steam Beer': 11,\n",
       " 'Chile Beer': 11,\n",
       " 'Cream Ale': 69,\n",
       " 'Czech Pilsener': 1501,\n",
       " 'Doppelbock': 873,\n",
       " 'Dortmunder / Export Lager': 31,\n",
       " 'Dubbel': 165,\n",
       " 'Dunkelweizen': 61,\n",
       " 'Eisbock': 8,\n",
       " 'English Barleywine': 133,\n",
       " 'English Bitter': 267,\n",
       " 'English Brown Ale': 495,\n",
       " 'English Dark Mild Ale': 21,\n",
       " 'English India Pale Ale (IPA)': 175,\n",
       " 'English Pale Ale': 1324,\n",
       " 'English Pale Mild Ale': 21,\n",
       " 'English Porter': 367,\n",
       " 'English Stout': 136,\n",
       " 'English Strong Ale': 164,\n",
       " 'Euro Dark Lager': 144,\n",
       " 'Euro Pale Lager': 701,\n",
       " 'Euro Strong Lager': 329,\n",
       " 'Extra Special / Strong Bitter (ESB)': 667,\n",
       " 'Flanders Oud Bruin': 13,\n",
       " 'Flanders Red Ale': 2,\n",
       " 'Foreign / Export Stout': 55,\n",
       " 'Fruit / Vegetable Beer': 1355,\n",
       " 'German Pilsener': 586,\n",
       " 'Hefeweizen': 618,\n",
       " 'Herbed / Spiced Beer': 73,\n",
       " 'Irish Dry Stout': 101,\n",
       " 'Irish Red Ale': 83,\n",
       " 'Keller Bier / Zwickel Bier': 23,\n",
       " 'Kristalweizen': 7,\n",
       " 'KÃ¶lsch': 94,\n",
       " 'Lambic - Fruit': 6,\n",
       " 'Lambic - Unblended': 10,\n",
       " 'Light Lager': 503,\n",
       " 'Low Alcohol Beer': 7,\n",
       " 'Maibock / Helles Bock': 225,\n",
       " 'Milk / Sweet Stout': 69,\n",
       " 'Munich Dunkel Lager': 141,\n",
       " 'Munich Helles Lager': 650,\n",
       " 'MÃ¤rzen / Oktoberfest': 557,\n",
       " 'Oatmeal Stout': 102,\n",
       " 'Old Ale': 1052,\n",
       " 'Pumpkin Ale': 560,\n",
       " 'Quadrupel (Quad)': 119,\n",
       " 'Rauchbier': 1938,\n",
       " 'Russian Imperial Stout': 2695,\n",
       " 'Rye Beer': 1798,\n",
       " 'Saison / Farmhouse Ale': 141,\n",
       " 'Schwarzbier': 53,\n",
       " 'Scotch Ale / Wee Heavy': 2776,\n",
       " 'Scottish Ale': 78,\n",
       " 'Scottish Gruit / Ancient Herbed Ale': 65,\n",
       " 'Smoked Beer': 61,\n",
       " 'Tripel': 257,\n",
       " 'Vienna Lager': 33,\n",
       " 'Weizenbock': 13,\n",
       " 'Wheatwine': 455,\n",
       " 'Winter Warmer': 259,\n",
       " 'Witbier': 162}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Altbier': 3.40303,\n",
       " 'American Adjunct Lager': 2.948347,\n",
       " 'American Amber / Red Ale': 3.513534,\n",
       " 'American Amber / Red Lager': 3.690476,\n",
       " 'American Barleywine': 4.064242,\n",
       " 'American Black Ale': 3.873188,\n",
       " 'American Blonde Ale': 3.254902,\n",
       " 'American Brown Ale': 3.743631,\n",
       " 'American Dark Wheat Ale': 3.678571,\n",
       " 'American Double / Imperial IPA': 4.033325,\n",
       " 'American Double / Imperial Pilsner': 3.821429,\n",
       " 'American Double / Imperial Stout': 4.479963,\n",
       " 'American IPA': 4.000851,\n",
       " 'American Malt Liquor': 2.255556,\n",
       " 'American Pale Ale (APA)': 3.649694,\n",
       " 'American Pale Lager': 3.215447,\n",
       " 'American Pale Wheat Ale': 3.334416,\n",
       " 'American Porter': 4.081839,\n",
       " 'American Stout': 3.819797,\n",
       " 'American Strong Ale': 3.569277,\n",
       " 'American Wild Ale': 4.188776,\n",
       " 'Baltic Porter': 4.213035,\n",
       " 'Belgian Dark Ale': 3.34,\n",
       " 'Belgian IPA': 3.949219,\n",
       " 'Belgian Pale Ale': 3.739583,\n",
       " 'Belgian Strong Dark Ale': 3.695205,\n",
       " 'Belgian Strong Pale Ale': 4.056171,\n",
       " 'Berliner Weissbier': 3.55,\n",
       " 'BiÃ¨re de Garde': 3.928571,\n",
       " 'Black & Tan': 3.942623,\n",
       " 'Bock': 3.189189,\n",
       " 'Braggot': 3.807692,\n",
       " 'California Common / Steam Beer': 3.318182,\n",
       " 'Chile Beer': 3.954545,\n",
       " 'Cream Ale': 3.028986,\n",
       " 'Czech Pilsener': 3.609594,\n",
       " 'Doppelbock': 3.982818,\n",
       " 'Dortmunder / Export Lager': 3.419355,\n",
       " 'Dubbel': 3.736364,\n",
       " 'Dunkelweizen': 3.491803,\n",
       " 'Eisbock': 3.75,\n",
       " 'English Barleywine': 4.360902,\n",
       " 'English Bitter': 3.537453,\n",
       " 'English Brown Ale': 3.728283,\n",
       " 'English Dark Mild Ale': 3.785714,\n",
       " 'English India Pale Ale (IPA)': 3.471429,\n",
       " 'English Pale Ale': 3.483761,\n",
       " 'English Pale Mild Ale': 3.595238,\n",
       " 'English Porter': 3.707084,\n",
       " 'English Stout': 3.599265,\n",
       " 'English Strong Ale': 3.756098,\n",
       " 'Euro Dark Lager': 3.704861,\n",
       " 'Euro Pale Lager': 2.96291,\n",
       " 'Euro Strong Lager': 2.848024,\n",
       " 'Extra Special / Strong Bitter (ESB)': 3.685157,\n",
       " 'Flanders Oud Bruin': 3.923077,\n",
       " 'Flanders Red Ale': 3.25,\n",
       " 'Foreign / Export Stout': 3.254545,\n",
       " 'Fruit / Vegetable Beer': 3.607749,\n",
       " 'German Pilsener': 3.667235,\n",
       " 'Hefeweizen': 3.635113,\n",
       " 'Herbed / Spiced Beer': 3.445205,\n",
       " 'Irish Dry Stout': 3.623762,\n",
       " 'Irish Red Ale': 2.981928,\n",
       " 'Keller Bier / Zwickel Bier': 3.869565,\n",
       " 'Kristalweizen': 2.785714,\n",
       " 'KÃ¶lsch': 3.696809,\n",
       " 'Lambic - Fruit': 3.75,\n",
       " 'Lambic - Unblended': 3.3,\n",
       " 'Light Lager': 2.39662,\n",
       " 'Low Alcohol Beer': 2.714286,\n",
       " 'Maibock / Helles Bock': 3.746667,\n",
       " 'Milk / Sweet Stout': 3.782609,\n",
       " 'Munich Dunkel Lager': 3.780142,\n",
       " 'Munich Helles Lager': 3.959231,\n",
       " 'MÃ¤rzen / Oktoberfest': 3.593357,\n",
       " 'Oatmeal Stout': 3.77451,\n",
       " 'Old Ale': 4.096008,\n",
       " 'Pumpkin Ale': 3.7875,\n",
       " 'Quadrupel (Quad)': 3.596639,\n",
       " 'Rauchbier': 4.067853,\n",
       " 'Russian Imperial Stout': 4.300371,\n",
       " 'Rye Beer': 4.213571,\n",
       " 'Saison / Farmhouse Ale': 3.702128,\n",
       " 'Schwarzbier': 3.622642,\n",
       " 'Scotch Ale / Wee Heavy': 4.083393,\n",
       " 'Scottish Ale': 3.762821,\n",
       " 'Scottish Gruit / Ancient Herbed Ale': 3.907692,\n",
       " 'Smoked Beer': 3.196721,\n",
       " 'Tripel': 3.784047,\n",
       " 'Vienna Lager': 3.530303,\n",
       " 'Weizenbock': 3.384615,\n",
       " 'Wheatwine': 4.186813,\n",
       " 'Winter Warmer': 3.621622,\n",
       " 'Witbier': 3.527778}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgRev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple predictor with a single binary feature indicating whether a beer is an `American IPA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFeature(data):\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in data:\n",
    "        if d['beer/style'] == 'American IPA':\n",
    "            X.append([1, 1])\n",
    "        else:\n",
    "            X.append([1, 0])\n",
    "        y.append(d['review/taste'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = genFeature(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Linear Regression using numpy OLS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, residuals, rank, s = np.linalg.lstsq(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta     : 3.9152047420838927 0.08564621828574305\n",
      "Residuals : [25650.80867309]\n"
     ]
    }
   ],
   "source": [
    "print(\"Theta     :\", theta[0], theta[1] )\n",
    "print(\"Residuals :\", residuals) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta(n_cols):\n",
    "    return np.random.randn(n_cols)\n",
    "\n",
    "def hypo_linReg(X, theta):\n",
    "    return np.dot(X, theta.T)\n",
    "\n",
    "def costFun_linReg(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = hypo_linReg(X, theta)\n",
    "    return  ( np.sum( (h - y) ** 2) ) / m \n",
    "\n",
    "def cal_gradient_linReg(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = hypo_linReg(X, theta)\n",
    "    return ( np.dot(X.T, ( h - y )) ) / m\n",
    "\n",
    "def perform_linReg(X, y, lr=0.01, num_itr=100000):\n",
    "    n_cols = X.shape[1]\n",
    "    theta = init_theta(n_cols)\n",
    "    costs = []\n",
    "    for i in range(num_itr):\n",
    "        gradient = cal_gradient_linReg(X, y, theta)\n",
    "        theta -= lr * gradient\n",
    "        if( i%100 == 0):\n",
    "            costs.append(costFun_linReg(X, y, theta))\n",
    "        if( i %10000 == 0):\n",
    "            print(costFun_linReg(X, y, theta))\n",
    "    return theta, costs\n",
    "\n",
    "def predict_linReg(X, theta):\n",
    "    y_pred = hypo_linReg(X, theta)\n",
    "    return y_pred\n",
    "\n",
    "def mse_linReg(y_true, y_pred):\n",
    "    m = len(y_true)\n",
    "    mse = np.sum( (y_true - y_pred) ** 2 ) / m\n",
    "    return mse\n",
    "\n",
    "def r2_linReg(y_true, y_pred):\n",
    "    y_mean = np.mean(y_true)\n",
    "    sse = np.sum( (y_true - y_pred) ** 2 )\n",
    "    tss = np.sum( (y_true - y_mean) ** 2 )\n",
    "    return (1 - sse/tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, costs = perform_linReg(X, y, lr=0.01, num_itr=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_linReg(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta    :  [3.91520474 0.08564622]\n",
      "MSE      :  0.5130161734617366\n",
      "R2_score :  0.0010782641230113743\n"
     ]
    }
   ],
   "source": [
    "print(\"Theta    : \", theta)\n",
    "print(\"MSE      : \", mse_linReg(y, y_pred))\n",
    "print(\"R2_score : \", r2_linReg(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHzlJREFUeJzt3XucHFWd9/HPNxlIALlnjCRcRuTiAkpgRwRRNwoisDzC8rBKZBUva8QXrPcHQXfVRd3FVdRVVIwQIopRuYqIXEQRUG6TLIHILeHik5BIJiI3uU7y2z/OGVK03dU1k/T0TOf7fr36NV1Vp6p+1ZX0r0+dqnMUEZiZmTUzrt0BmJnZ2OCEYWZmlThhmJlZJU4YZmZWiROGmZlV4oRhZmaVOGHYekvSHEmfb8F2j5F05bre7mgj6feSprc7Dhs5Thi21iS9XVKfpCckLZf0C0mvXcttPiDpwHUVY6tI6pEUkroG50XEuRFxUDvjWtfqJdeI2D0irmlTSNYGThi2ViR9FPga8B/AZGB74FvA4SMcR1fzUlaFP0trxAnDhk3S5sApwPERcWFE/CUinouIn0XE/8tlJkj6mqRl+fU1SRPyskmSLpX0iKSHJV0naZyk75MSz89yreXEOvueLmmppE9I+iNwdp5/mKRb8zZ/J+mVhXX2kjRf0uOSfgxMLCx7l6Tra/YRknbK7zeSdJqkP0h6VNL1kjYCrs3FH8mx7le7LUmvkXRLXu8WSa8pLLtG0uck/TbHdaWkSSWf+fskLc6f1yWSpuT5Z0j6ck3Zn+aEjqQpki6Q1C/pfkkfLJT7rKTzJf1A0mPAu2q2MxM4BjgxH+PP8vzna4F5G+flbTwu6XZJu0g6WdIKSUskHVTY5uaSzso10gclfV7S+EbHbaNERPjl17BewMHAANBVUuYU4EbgxUA38Dvgc3nZfwJnABvk1+sA5WUPAAeWbHd63vcXgQnARsDewArg1cB44Ni8nQnAhsAfgI/kfR0FPAd8Pm/vXcD1NfsIYKf8/pvANcDUvO3X5O325HJdhfWe3xawFfBn4B1AFzAjT2+dl18D3Avsko/hGuDUBsf8RmBlPs4JwDeAa/Oy1wNLCp/flsBTwBTSD8N5wKfz57AjcB/w5lz2s/mzOCKX3ajOvucMflaFec+fo7yNp4E35+M8B7gf+FT+vN8H3F9Y92LgO8AmpH8bNwPvb/e/ab/KX65h2NrYGlgZEQMlZY4BTomIFRHRD/w76csT0pfUNsAOkWom10X+NqloNfCZiHgmIp4ifSl9JyJuiohVEfE94Blg3/zaAPha3tf5wC1VdiJpHPAe4EMR8WDe9u8i4pkKq/89sCgivh8RAxExF7gL+D+FMmdHxD35GH4CTGuwrWOA2RExP+/7ZGA/ST3AdaTE9bpc9ijghohYBrwK6I6IUyLi2Yi4D/gucHRh2zdExMURsTrHMRzXRcQV+d/DeaQfCKdGxHPAj4AeSVtImgwcAnw4Uq10BfDVmnhsFHLCsLXxJ2BSk2veU0i/7Af9Ic8D+BKwGLhS0n2SThri/vsj4unC9A7Ax/LlqEckPQJsl/c3BXiwJiEV4yoziXT56t4hxgd/ffyD+51amP5j4f2TwIuqbCsiniCdg6n5uH5EqsEAvB04N7/fAZhS87l8ktTmNGhJ5SNq7KHC+6dIPyZWFaYhHdsOpOS9vBDPd0g1DRvFnDBsbdxAugxxREmZZaQviEHb53lExOMR8bGI2JH0i/ujkg7I5arUNGrLLAG+EBFbFF4b51/1y4GpklQTy6C/ABsPTkh6SWHZStJxvqxCDLVqj39wvw82Wa/ptiRtQqrlDW5rLnCUpB1Il+UuyPOXkC4HFT+XTSPi0CEcx7rs1noJqeY3qRDPZhGx+zrch7WAE4YNW0Q8Srou/k1JR0jaWNIGkg6R9F+52FzgXyV158bcTwM/gOcbqHfKX+KPAavyC9Kv1R2HGNJ3geMkvVrJJpL+XtKmpOQ2AHxQUpekI4F9CusuAHaXNE3SRNI1+cHjXA3MBr6SG4/H58btCUA/6dJYo1gvA3ZRuvW4S9LbgN2AS4d4bAA/BN6dY5xAujPtpoh4IMf5PzmeM4ErIuKRvN7NwGNKNwhslOPfQ9KrhrDv4ZyPuiJiOXAlcJqkzZRudHiZpL9bF9u31nHCsLUSEV8BPgr8K+nLaglwAqlRE+DzQB9wG3A7MD/PA9gZ+CXwBOkL/Vux5r7+/yQlmkckfbxiLH2kdozTSQ3Li8l3/ETEs8CRefrPwNuACwvr3kNqoP8lsAh4wR1TwMdz/LcAD5Ma28dFxJPAF4Df5lj3rYnpT8BhwMdIl49OBA6LiJVVjqlmW1cD/0aqOSwn1Xhqr/vPBQ4kJZfB9VaRanDTSA3RK0lJZfMh7P4sYLd8jBc3Ld3cO0kN8HeQzsf5pPYsG8UG76gwMzMr5RqGmZlV4oRhZmaVOGGYmVklThhmZlZJR3UyNmnSpOjp6Wl3GGZmY8a8efNWRkR3lbIdlTB6enro6+trdxhmZmOGpKo9HviSlJmZVeOEYWZmlThhmJlZJU4YZmZWiROGmZlV4oRhZmaVOGGYmVklThjAN65exG/u6W93GGZmo5oTBvCta+7lt4uHPDyBmdl6xQkj87ggZmblnDCAF4zybGZmdTlhZK5gmJmVc8IAXMEwM2vOCcPMzCpxwsh8RcrMrJwTBiC3epuZNeWEkbnR28ysnBMGbvQ2M6vCCSMLt2KYmZVq2ZjekmYDhwErImKPPO/HwK65yBbAIxExrc66DwCPA6uAgYjobVWcaYct3bqZWUdoWcIA5gCnA+cMzoiItw2+l3Qa8GjJ+m+IiBHr4MltGGZm5VqWMCLiWkk99ZYp3Zb0VuCNrdr/ULiCYWbWXLvaMF4HPBQRixosD+BKSfMkzSzbkKSZkvok9fX3u4tyM7NWaVfCmAHMLVm+f0TsDRwCHC/p9Y0KRsSsiOiNiN7u7u5hBePnMMzMmhvxhCGpCzgS+HGjMhGxLP9dAVwE7DMy0ZmZWSPtqGEcCNwVEUvrLZS0iaRNB98DBwELWx2Ux8MwMyvXsoQhaS5wA7CrpKWS3psXHU3N5ShJUyRdlicnA9dLWgDcDPw8Ii5vVZxp/63cuplZZ2jlXVIzGsx/V515y4BD8/v7gD1bFVcjrl+YmZXzk974tlozsyqcMDI3YZiZlXPCwLfVmplV4YSRufNBM7NyThi4DcPMrAonjMxtGGZm5Zww8HMYZmZVOGGYmVklThiZr0iZmZVzwgDc7G1m1pwTRuZGbzOzck4YuNHbzKwKJ4znuYphZlbGCQO3YJiZVeGEkbkNw8ysnBMGbsMwM6vCCcPMzCpxwsh8ScrMrFwrx/SeLWmFpIWFeZ+V9KCkW/Pr0AbrHizpbkmLJZ3Uqhif35+bvc3MmmplDWMOcHCd+V+NiGn5dVntQknjgW8ChwC7ATMk7dbCOAGPh2Fm1kzLEkZEXAs8PIxV9wEWR8R9EfEs8CPg8HUaXA03epuZNdeONowTJN2WL1ltWWf5VGBJYXppnleXpJmS+iT19ff3Dzsot2GYmZUb6YTxbeBlwDRgOXBanTL1fu83/DqPiFkR0RsRvd3d3cMKyhUMM7PmRjRhRMRDEbEqIlYD3yVdfqq1FNiuML0tsKzlsbV6B2ZmY9yIJgxJ2xQm/wFYWKfYLcDOkl4qaUPgaOCSFsfVys2bmXWErlZtWNJcYDowSdJS4DPAdEnTSD/oHwDen8tOAc6MiEMjYkDSCcAVwHhgdkT8vlVxDnIbhplZuZYljIiYUWf2WQ3KLgMOLUxfBvzVLbdmZtY+ftLbzMwqccLI/OCemVk5Jwz84J6ZWRVOGINcwTAzK+WEgWsYZmZVOGFkrmCYmZVzwsDdm5uZVeGEkYWf3DMzK+WEgdswzMyqcMLIXL8wMyvnhIG7Nzczq8IJw8zMKnHCyNzmbWZWzgkDj4dhZlaFE0bmCoaZWTknDNzobWZWhRNG5gf3zMzKOWGAqxhmZhW0LGFImi1phaSFhXlfknSXpNskXSRpiwbrPiDpdkm3SuprVYxFrl+YmZVrZQ1jDnBwzbyrgD0i4pXAPcDJJeu/ISKmRURvi+J7nisYZmbNtSxhRMS1wMM1866MiIE8eSOwbav2P2SuYpiZlWpnG8Z7gF80WBbAlZLmSZpZthFJMyX1Serr7+8fViB+DsPMrLm2JAxJnwIGgHMbFNk/IvYGDgGOl/T6RtuKiFkR0RsRvd3d3S2I1szMoA0JQ9KxwGHAMdHgXtaIWJb/rgAuAvZpdVzha1JmZqVGNGFIOhj4BPCWiHiyQZlNJG06+B44CFhYr+w6i6uVGzcz6xCtvK12LnADsKukpZLeC5wObApclW+ZPSOXnSLpsrzqZOB6SQuAm4GfR8TlrYpzkJ/bMzMr19WqDUfEjDqzz2pQdhlwaH5/H7Bnq+Kqx23eZmbN+UnvzDUMM7NyThiA3IphZtaUE0bmu6TMzMo5YeA2DDOzKpwwMrdhmJmVc8IwM7NKnDDMzKwSJ4zMV6TMzMo5YeDeas3MqnDCyNzobWZWzgkDdz5oZlaFE8bzXMUwMyvjhIEf3DMzq8IJI3MbhplZOScMXMMwM6vCCSNzBcPMrJwTBu7e3MysikoJQ9I/VplnZmadq2oN4+SK815A0mxJKyQtLMzbStJVkhblv1s2WPfYXGaRpGMrxjls4VZvM7NSpWN6SzqENNb2VElfLyzaDBiosP05wOnAOYV5JwFXR8Spkk7K05+o2e9WwGeAXlLzwjxJl0TEnyvsc8jc6G1m1lyzGsYyoA94GphXeF0CvLnZxiPiWuDhmtmHA9/L778HHFFn1TcDV0XEwzlJXAUc3Gx/a8P1CzOzcqU1jIhYACyQ9MOIeA4gX0Labi1+7U+OiOV5+8slvbhOmanAksL00jzvr0iaCcwE2H777YcVkCsYZmbNVW3DuErSZvlS0QLgbElfaWFc9b7D61YCImJWRPRGRG93d/ewd+gmDDOzclUTxuYR8RhwJHB2RPwtcOAw9/mQpG0A8t8VdcosBbYrTG9LujzWGm7EMDNrqmrC6Mpf7m8FLl3LfV4CDN71dCzw0zplrgAOkrRlvgR2UJ7XMq5gmJmVq5owTiF9Yd8bEbdI2hFY1GwlSXOBG4BdJS2V9F7gVOBNkhYBb8rTSOqVdCZARDwMfA64Jb9OyfNawvULM7PmShu9B0XEecB5hen7gP9bYb0ZDRYdUKdsH/DPhenZwOwq8a0Lfg7DzKxc1Se9t5V0UX4I7yFJF0jattXBmZnZ6FH1ktTZpLaHKaTbW3+W53UEt3mbmTVXNWF0R8TZETGQX3OA4d/DamZmY07VhLFS0j9JGp9f/wT8qZWBjSRXMMzMmquaMN5DuqX2j8By4Cjg3a0Kqh3c5m1mVq7SXVKkW1yPHewOJD/x/WVSIhnz5EYMM7OmqtYwXlnsOyo/E7FXa0Jqj/Cje2ZmpaomjHHFcStyDaNq7WTUc/3CzKy5ql/6pwG/k3Q+qReNtwJfaFlUbeA2DDOzclWf9D5HUh/wRtIP8iMj4o6WRjaC3IRhZtZc5ctKOUF0TJIwM7OhqdqG0fF8ScrMrJwTBiA3e5uZNeWEkfm2WjOzck4Y4PtqzcwqcMLI3IZhZlbOCQNXMMzMqnDCyFzBMDMrN+IJQ9Kukm4tvB6T9OGaMtMlPVoo8+nWxtTKrZuZdYYR7w8qIu4GpgFIGg88CFxUp+h1EXHYyAU2YnsyMxuT2n1J6gDg3oj4QzuD8HMYZmbNtTthHA3MbbBsP0kLJP1C0u6NNiBppqQ+SX39/f2tidLMzNqXMCRtCLwFOK/O4vnADhGxJ/AN4OJG24mIWRHRGxG93d3DH2bcD+6ZmZVrZw3jEGB+RDxUuyAiHouIJ/L7y4ANJE1qVSBu9DYza66dCWMGDS5HSXqJ8ripkvYhxfmnVgbjB/fMzMq1ZdQ8SRsDbwLeX5h3HEBEnAEcBXxA0gDwFHB0ROu+0l3DMDNrri0JIyKeBLaumXdG4f3pwOkjGtNI7szMbAxq911So4JvqzUza84JI2vhFS8zs47ghIHbMMzMqnDCyFy/MDMr54RhZmaVOGGYmVklThiZ27zNzMo5YQByq7eZWVNOGJkrGGZm5Zww8JjeZmZVOGEMciOGmVkpJwz84J6ZWRVOGJnrF2Zm5ZwwcBuGmVkVThiZmzDMzMo5YeDnMMzMqnDCMDOzSpwwsnCzt5lZqbYlDEkPSLpd0q2S+uosl6SvS1os6TZJe7csllZt2Mysg7RlTO+CN0TEygbLDgF2zq9XA9/Of1vCjd5mZuVG8yWpw4FzIrkR2ELSNq3Ykdu8zcyaa2fCCOBKSfMkzayzfCqwpDC9NM97AUkzJfVJ6uvv7x9+MK5hmJmVamfC2D8i9iZdejpe0utrltf73f9XX+sRMSsieiOit7u7e5ihuIphZtZM2xJGRCzLf1cAFwH71BRZCmxXmN4WWNayeFq1YTOzDtGWhCFpE0mbDr4HDgIW1hS7BHhnvltqX+DRiFjemnhasVUzs87SrrukJgMX5Sesu4AfRsTlko4DiIgzgMuAQ4HFwJPAu1sZULgRw8ysVFsSRkTcB+xZZ/4ZhfcBHD8S8biCYWbW3Gi+rdbMzEYRJwwzM6vECQM3epuZVeGEkbnN28ysnBMGIDd7m5k15YSRuXtzM7NyThi4DcPMrAonjMxtGGZm5ZwwcA3DzKwKJ4zMFQwzs3JOGGZmVokTBr6t1sysCieMzL3VmpmVc8IAd1drZlaBE0bm+oWZWTknDFzBMDOrwgljkKsYZmalnDCA8ePEKjd6m5mVGvGEIWk7Sb+WdKek30v6UJ0y0yU9KunW/Pp0K2MaP06sWu2EYWZWph1jeg8AH4uI+ZI2BeZJuioi7qgpd11EHDYSAY2XE4aZWTMjXsOIiOURMT+/fxy4E5g60nEUdY13wjAza6atbRiSeoC9gJvqLN5P0gJJv5C0e8k2Zkrqk9TX398/rDjGuYZhZtZU2xKGpBcBFwAfjojHahbPB3aIiD2BbwAXN9pORMyKiN6I6O3u7h5WLG70NjNrri0JQ9IGpGRxbkRcWLs8Ih6LiCfy+8uADSRNalU848eJVaucMMzMyrTjLikBZwF3RsRXGpR5SS6HpH1Icf6pVTGNl2sYZmbNtOMuqf2BdwC3S7o1z/sksD1ARJwBHAV8QNIA8BRwdLSwd8DxbvQ2M2tqxBNGRFxPk944IuJ04PSRici31ZqZVeEnvYEuN3qbmTXlhAGMGyciYLVrGWZmDTlhkC5JAa5lmJmVcMIgNXoDbscwMyvhhEGhhuGEYWbWkBMG6cE9gAEnDDOzhpwwWJMw3OhtZtaYEwbptlpwo7eZWRknDNJtteA2DDOzMk4YFGoYThhmZg05YZDGwwAnDDOzMk4YrGn0dsIwM2vMCQOYuMF4AJ58dlWbIzEzG72cMIAXbzoBgP4nnmlzJGZmo5cTBjB5s4kALH/kqTZHYmY2erVjAKVRZ/JmE9lsYhcnXXg7X7z8LrrGj2ODcaJr/Lh0B1XJ6B2lA3sAeeDAYa1rZlbFlhtvyE+O26/l+3HCADbsGse5/7wvv1i4nMefHmBg9WqeWxUMrFpd2l1I0ybykgLRfG0zs0o2m7jBiOzHCSN7xbab84ptN293GGZmo1Zb2jAkHSzpbkmLJZ1UZ/kEST/Oy2+S1DPyUZqZWdGIJwxJ44FvAocAuwEzJO1WU+y9wJ8jYifgq8AXRzZKMzOr1Y4axj7A4oi4LyKeBX4EHF5T5nDge/n9+cABKms9NjOzlmtHwpgKLClML83z6paJiAHgUWDrehuTNFNSn6S+/v7+FoRrZmbQnoRRr6ZQe8tQlTJpZsSsiOiNiN7u7u61Ds7MzOprR8JYCmxXmN4WWNaojKQuYHPg4RGJzszM6mpHwrgF2FnSSyVtCBwNXFJT5hLg2Pz+KOBXER7dyMysnUb8OYyIGJB0AnAFMB6YHRG/l3QK0BcRlwBnAd+XtJhUszh6pOM0M7MXUif9cJfUD/xhmKtPAlauw3DGAh/z+sHH3PnW5nh3iIhKDcAdlTDWhqS+iOhtdxwjyce8fvAxd76ROl73VmtmZpU4YZiZWSVOGGvMancAbeBjXj/4mDvfiByv2zDMzKwS1zDMzKwSJwwzM6tkvU8YzcbmGKskbSfp15LulPR7SR/K87eSdJWkRfnvlnm+JH09fw63Sdq7vUcwfJLGS/ofSZfm6ZfmcVUW5XFWNszzO2LcFUlbSDpf0l35fO/X6edZ0kfyv+uFkuZKmthp51nSbEkrJC0szBvyeZV0bC6/SNKx9fZV1XqdMCqOzTFWDQAfi4i/AfYFjs/HdhJwdUTsDFydpyF9Bjvn10zg2yMf8jrzIeDOwvQXga/mY/4zabwV6JxxV/4buDwiXg7sSTr2jj3PkqYCHwR6I2IPUo8RR9N553kOcHDNvCGdV0lbAZ8BXk0aWuIzg0lmWCJivX0B+wFXFKZPBk5ud1wtOtafAm8C7ga2yfO2Ae7O778DzCiUf77cWHqROrO8GngjcCmp5+OVQFftOSd1T7Nfft+Vy6ndxzDE490MuL827k4+z6wZ/mCrfN4uBd7ciecZ6AEWDve8AjOA7xTmv6DcUF/rdQ2DamNzjHm5Cr4XcBMwOSKWA+S/L87FOuWz+BpwIrA6T28NPBJpXBV44XFVHndlFNsR6AfOzpfhzpS0CR18niPiQeDLwP8HlpPO2zw6+zwPGup5Xafne31PGJXH3RirJL0IuAD4cEQ8Vla0zrwx9VlIOgxYERHzirPrFI0Ky8aKLmBv4NsRsRfwF9ZcpqhnzB9zvqRyOPBSYAqwCemSTK1OOs/NNDrGdXrs63vCqDI2x5glaQNSsjg3Ii7Msx+StE1evg2wIs/vhM9if+Atkh4gDf37RlKNY4s8rgq88Lg6YdyVpcDSiLgpT59PSiCdfJ4PBO6PiP6IeA64EHgNnX2eBw31vK7T872+J4wqY3OMSZJE6ib+zoj4SmFRcayRY0ltG4Pz35nvttgXeHSw6jtWRMTJEbFtRPSQzuWvIuIY4NekcVXgr495TI+7EhF/BJZI2jXPOgC4gw4+z6RLUftK2jj/Ox885o49zwVDPa9XAAdJ2jLXzA7K84an3Y067X4BhwL3APcCn2p3POvwuF5LqnreBtyaX4eSrt1eDSzKf7fK5UW6Y+xe4HbSHShtP461OP7pwKX5/Y7AzcBi4DxgQp4/MU8vzst3bHfcwzzWaUBfPtcXA1t2+nkG/h24C1gIfB+Y0GnnGZhLaqN5jlRTeO9wzivwnnzsi4F3r01M7hrEzMwqWd8vSZmZWUVOGGZmVokThpmZVeKEYWZmlThhmJlZJU4YNqIk/S7/7ZH09nW87U/W21erSDpC0qdbtO0nWrTd6YO9+K7FNuZIOqpk+QmS3r02+7DRyQnDRlREvCa/7QGGlDBy78JlXpAwCvtqlROBb63tRiocV8sVnpBeF2aTepO1DuOEYSOq8Mv5VOB1km7NYxuMl/QlSbfk/vzfn8tPVxrX44ekB5KQdLGkeXk8hJl53qnARnl75xb3lZ9+/VIeO+F2SW8rbPsarRlL4tz85DCSTpV0R47ly3WOYxfgmYhYmafnSDpD0nWS7sn9Wg2OzVHpuOrs4wuSFki6UdLkwn6OKpR5orC9RsdycJ53PXBkYd3PSpol6UrgnJJYJen0/Hn8nDUd3tX9nCLiSeABSftU+TdhY8e6/FVhNhQnAR+PiMEv1pmk7gxeJWkC8Nv8RQapH/89IuL+PP2eiHhY0kbALZIuiIiTJJ0QEdPq7OtI0tPQewKT8jrX5mV7AbuT+tf5LbC/pDuAfwBeHhEhaYs629wfmF8zrwf4O+BlwK8l7QS8cwjHVbQJcGNEfErSfwHvAz5fp1xRvWPpA75L6ldrMfDjmnX+FnhtRDxVcg72AnYFXgFMJnXDMVtprIVGn1Mf8DrSk9XWIVzDsNHiIFJfOLeSumHfmjQYDMDNNV+qH5S0ALiR1LHazpR7LTA3IlZFxEPAb4BXFba9NCJWk7pP6QEeA54GzpR0JPBknW1uQ+pWvOgnEbE6IhYB9wEvH+JxFT1LGucBUtfdPU2OsdGxvJzUUd+iSN06/KBmnUsi4qn8vlGsr2fN57cM+FUuX/Y5rSD1JGsdxDUMGy0E/EtEvKBjNEnTSV12F6cPJA2I86Ska0h9BTXbdiPPFN6vIg3AM5AvpxxA6sTwBNIv9KKnSL2eFtX2szPYvXTT46rjuVjTb88q1vxfHSD/0MuXnDYsO5YGcRUVY2gU66H1ttHkc5pI+oysg7iGYe3yOLBpYfoK4ANKXbIjaRelgYBqbU4abvNJSS8nDT876LnB9WtcC7wtX6PvJv1ibnipRGkMkc0j4jLgw6TLWbXuBHaqmfePksZJehmpI7y7h3BcVT1AuowEaUyIesdbdBfw0hwTpBHYGmkU67XA0fnz2wZ4Q15e9jntQuoY0DqIaxjWLrcBA/nS0hzSuNQ9wPz8y7kfOKLOepcDx0m6jfSFfGNh2SzgNknzI3VrPugi0pCdC0i/lE+MiD/mhFPPpsBPJU0k/er+SJ0y1wKnSVKhJnA36XLXZOC4iHha0pkVj6uq7+bYbib1VlpWSyHHMBP4uaSVwPXAHg2KN4r1IlLN4XZSz86/yeXLPqf9ST3KWgdxb7VmwyTpv4GfRcQvJc0hdad+fpvDajtJewEfjYh3tDsWW7d8Scps+P4D2LjdQYxCk4B/a3cQtu65hmFmZpW4hmFmZpU4YZiZWSVOGGZmVokThpmZVeKEYWZmlfwv1AQHdEaS51sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title('Cost reduction over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Linear Regression using sckit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coeficients : [0.08564622]\n",
      "intercept   : 3.9152047420838145\n",
      "MSE         : 0.5130161734617366\n",
      "r2_score    : 0.0010782641230113743\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X[:, 1:], y)\n",
    "y_pred = reg.predict(X[:, 1:])\n",
    "\n",
    "print(\"coeficients :\",reg.coef_)\n",
    "print(\"intercept   :\",reg.intercept_)\n",
    "print(\"MSE         :\",MSE(y, y_pred ))\n",
    "print(\"r2_score    :\",r2_score(y, y_pred ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theta[0] represent the y-intercept of regression line\n",
    "\n",
    "theta[1] represent the slope of regression line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Splitting the data into train-test sets , Finding MSE on training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:int(len(X)/2),1:], X[int(len(X)/2):,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y[:int(len(X)/2)], y[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Multile Regression using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coeficients:  [0.05606027]\n",
      "intercept:  3.9043563922942206\n",
      "MSE on train set:  0.558107286558669\n",
      "MSE on test set:  0.46841005096664573\n",
      "r2_score on train set: 0.0004442931134949202\n",
      "r2_score on test set: 7.227980262203282e-05\n"
     ]
    }
   ],
   "source": [
    "# linear Regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "print(\"coeficients: \",reg.coef_)\n",
    "print(\"intercept: \",reg.intercept_)\n",
    "print(\"MSE on train set: \",MSE(y_train, reg.predict(X_train)) )\n",
    "print(\"MSE on test set: \",MSE(y_test, reg.predict(X_test)))\n",
    "print(\"r2_score on train set:\",r2_score(y_train, reg.predict(X_train)) )\n",
    "print(\"r2_score on test set:\",r2_score(y_test, reg.predict(X_test)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extending the above model , incorporating binary features for every style of beer with ≥ 50 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFeatures(data, revFreq):\n",
    "    beer_rev50 = []\n",
    "    for beer_style in revFreq.keys():\n",
    "        if revFreq[beer_style] > 50:\n",
    "            beer_rev50.append(beer_style)\n",
    "\n",
    "    #print( \"beer styles with reviews count > 50 are :\",len(beer_rev50))\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in data:\n",
    "        x = []\n",
    "        for beer in beer_rev50:\n",
    "            x.append( d['beer/style'] == beer )\n",
    "        X.append(x)\n",
    "        y.append(d['review/taste'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = genFeatures(data, revFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 74), (50000,))"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:int(len(X)/2),:], X[int(len(X)/2):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y[:int(len(X)/2)], y[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Regression using Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.466161174215785\n",
      "1.5490885894803492\n",
      "0.9430198663226093\n",
      "0.7273682913968028\n",
      "0.6307755984632287\n",
      "0.5809299166878924\n",
      "0.5523048541673914\n",
      "0.5344712248326355\n",
      "0.5226539191642804\n",
      "0.5144452282235812\n"
     ]
    }
   ],
   "source": [
    "theta, costs = perform_linReg(X_train, y_train, lr=0.01, num_itr=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta                 : [2.16067365 3.70496231 3.71880122 2.71213301 3.63421292 2.33431577\n",
      " 3.01349467 3.60097688 3.27417137 3.62300486 2.17533747 4.09087111\n",
      " 4.23561594 2.77352253 3.74666144 3.39813055 0.22558877 3.06494389\n",
      " 0.98468172 2.68658586 3.57704447 4.30245993 3.41188565 3.87390864\n",
      " 3.45461356 2.2588773  4.06320225 2.46185677 3.57320825 3.47096659\n",
      " 3.15159761 3.57563592 3.94278607 4.0922619  3.95347531 2.00077233\n",
      " 3.36157604 0.33766567 3.96041667 3.28607378 3.50844877 3.264106\n",
      " 3.14865736 3.87378629 3.49047595 3.38924058 3.75011194 3.1762208\n",
      " 3.63327154 3.59017125 3.92288    2.71761546 3.80994151 4.44843488\n",
      " 1.73576346 2.85952762 3.39750714 2.89522068 3.67866154 3.66430649\n",
      " 3.93741678 3.71970596 3.25472511 1.79456744 3.73249376 2.51111882\n",
      " 1.54122883 2.54495123 4.07524272 3.720022   3.91330349 3.33223932\n",
      " 1.6709058  2.37291665]\n",
      "MSE on train set      : 0.5085291066874872\n",
      "R2_score on train set : 0.0892375302218158\n",
      "MSE on test set       : 1.0439487115649964\n",
      "R2_score on test set  : -1.2285458072557636\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_linReg(X_train, theta)\n",
    "print(\"Theta                 :\", theta)\n",
    "print(\"MSE on train set      :\", mse_linReg(y_train, y_pred))\n",
    "print(\"R2_score on train set :\", r2_linReg(y_train, y_pred) )\n",
    "y_pred = predict_linReg(X_test, theta)\n",
    "print(\"MSE on test set       :\", mse_linReg(y_test, y_pred))\n",
    "print(\"R2_score on test set  :\", r2_linReg(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Multiple Linear Regression Using scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coeficients:  [ 0.71136364  0.09815076  0.12193999 -0.4664673   0.02739474  0.12395105\n",
      "  0.22651515 -0.0058248  -0.2937747   0.01699134 -0.50681818  0.58195733\n",
      "  0.76540404 -0.83277972  0.14466111  0.11433566  0.39318182  0.26818182\n",
      "  0.25681818 -0.92019846 -0.02450111  0.69564175 -0.1798951   0.26709486\n",
      " -0.10223103  0.01818182  0.45638407 -0.58598485  0.10049889 -0.13106061\n",
      " -0.45410882  0.38356643  0.33596789  0.48544372  0.3763751  -0.65227273\n",
      "  0.44318182 -0.35681818  0.35359848  0.60472028 -0.09003966 -0.22045455\n",
      " -0.17824675  0.26761104 -0.11634199  0.1527972   0.16385851 -0.41285266\n",
      "  0.03420746  0.19621212  0.33580477 -0.74318182  0.20312334  0.8416167\n",
      " -0.10681818 -0.73802386 -0.18884943 -0.63806818  0.13786267  0.05895722\n",
      "  0.3305986   0.12651515 -0.31931818 -1.00681818  0.20984848  0.65508658\n",
      " -0.20681818 -0.38622995  0.46842454  0.11320382  0.3449362  -0.27061129\n",
      " -0.62765152 -1.23390152]\n",
      "intercept:  3.606818181818188\n",
      "MSE on training set:  0.3678402770901444\n",
      "MSE on testing set:  0.4336695104199656\n",
      "r2_score on train set: 0.3412075831238651\n",
      "r2_score on test set: 0.07423385988737574\n"
     ]
    }
   ],
   "source": [
    "# linear Regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "print(\"coeficients          : \",reg.coef_)\n",
    "print(\"intercept            : \",reg.intercept_)\n",
    "\n",
    "y_pred = reg.predict(X_train)\n",
    "print(\"MSE on train set     : \",MSE(y_train, reg.predict(X_train)) )\n",
    "print(\"r2_score on train set:\",r2_score(y_train, y_pred) )\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"MSE on test set      : \",MSE(y_test, reg.predict(X_test)) )\n",
    "print(\"r2_score on test set :\",r2_score(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks — Classification (week 2):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. A predictor that estimates whether a beer is an `American IPA` using two features:\n",
    "[`beer/ABV`,`review/taste`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ABV_taste(datum):\n",
    "    return [1, datum['beer/ABV'], datum['review/taste']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature_ABV_taste(d) for d in data]\n",
    "y = [d['beer/style'] == 'American IPA' for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:int(len(X)/2)], X[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y[:int(len(y)/2)], y[int(len(y)/2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(C=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy score :  0.9226\n",
      "confusion matrix :\n",
      " [[22594   246]\n",
      " [ 1689   471]]\n",
      "classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.93      0.99      0.96     22840\n",
      "       True       0.66      0.22      0.33      2160\n",
      "\n",
      "avg / total       0.91      0.92      0.90     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, y_pred )\n",
    "print(\"Train set accuracy score : \", train_acc )\n",
    "print(\"confusion matrix :\\n\", confusion_matrix(y_train, y_pred))\n",
    "print(\"classification report :\\n\",classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy score : 0.85632\n",
      "confusion matrix : \n",
      " [[21364  1683]\n",
      " [ 1909    44]]\n",
      "classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.92      0.93      0.92     23047\n",
      "       True       0.03      0.02      0.02      1953\n",
      "\n",
      "avg / total       0.85      0.86      0.85     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy score :\", test_acc )\n",
    "print(\"confusion matrix : \\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"classification report :\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Engineering using common words in `review/text` of   `American IPA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "textRevAmericanIPA = []\n",
    "for d in data:\n",
    "    if d['beer/style'] == 'American IPA':\n",
    "        textRevAmericanIPA.append(d['review/text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = []\n",
    "for text in textRevAmericanIPA:\n",
    "    tokens += tokenizer.tokenize(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "all_words = [w for w in tokens if not w in stop_words and len(w) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDist = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hops', 4686),\n",
       " ('head', 3942),\n",
       " ('The', 3872),\n",
       " ('beer', 3804),\n",
       " ('IPA', 3663),\n",
       " ('hop', 3343),\n",
       " ('malt', 3117),\n",
       " ('nice', 2959),\n",
       " ('citrus', 2865),\n",
       " ('good', 2613),\n",
       " ('one', 2040),\n",
       " ('orange', 1916),\n",
       " ('like', 1916),\n",
       " ('white', 1876),\n",
       " ('carbonation', 1825),\n",
       " ('color', 1799),\n",
       " ('glass', 1799),\n",
       " ('taste', 1772),\n",
       " ('This', 1760),\n",
       " ('grapefruit', 1754),\n",
       " ('well', 1745),\n",
       " ('bitterness', 1734),\n",
       " ('pine', 1700),\n",
       " ('bitter', 1695),\n",
       " ('bit', 1690),\n",
       " ('flavor', 1684),\n",
       " ('lacing', 1660),\n",
       " ('light', 1660),\n",
       " ('finish', 1543),\n",
       " ('aroma', 1541),\n",
       " ('sweet', 1508),\n",
       " ('little', 1448),\n",
       " ('amber', 1339),\n",
       " ('caramel', 1319),\n",
       " ('body', 1292),\n",
       " ('Very', 1235),\n",
       " ('medium', 1179),\n",
       " ('bottle', 1151),\n",
       " ('Pours', 1141),\n",
       " ('floral', 1132),\n",
       " ('hoppy', 1102),\n",
       " ('great', 1048),\n",
       " ('nose', 1044),\n",
       " ('really', 1036),\n",
       " ('balanced', 1029),\n",
       " ('notes', 1014),\n",
       " ('smell', 951),\n",
       " ('dry', 926),\n",
       " ('fresh', 915),\n",
       " ('would', 909)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqDist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feat = ['citrus', 'IPA', 'bitter', 'grapefruit','caramel']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customFeatures(datum):\n",
    "    return [1, \n",
    "            datum['beer/ABV'],\n",
    "           'citrus' in datum['review/text'],\n",
    "           'IPA' in datum['review/text'],\n",
    "           'bitter' in datum['review/text'],\n",
    "           'grapefruit' in datum['review/text'],\n",
    "           'caramel' in datum['review/text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [customFeatures(d) for d in data]\n",
    "y = [d['beer/style'] == 'American IPA' for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:int(len(X)/2)], X[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y[:int(len(X)/2)], y[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = svm.SVC(C=1000)\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy score :  0.96248\n",
      "confusion matrix :\n",
      " [[22353   487]\n",
      " [  451  1709]]\n",
      "classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      0.98      0.98     22840\n",
      "       True       0.78      0.79      0.78      2160\n",
      "\n",
      "avg / total       0.96      0.96      0.96     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf2.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, y_pred )\n",
    "\n",
    "print(\"Train set accuracy score : \", train_acc )\n",
    "print(\"confusion matrix :\\n\", confusion_matrix(y_train, y_pred))\n",
    "print(\"classification report :\\n\",classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy score : 0.9262\n",
      "confusion matrix : \n",
      " [[21729  1318]\n",
      " [  527  1426]]\n",
      "classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      0.94      0.96     23047\n",
      "       True       0.52      0.73      0.61      1953\n",
      "\n",
      "avg / total       0.94      0.93      0.93     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf2.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy score :\", test_acc )\n",
    "print(\"confusion matrix : \\n\",confusion_matrix(y_test, y_pred))\n",
    "print(\"classification report :\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Effect  of  regularization  constant C  on SVM  performance \n",
    "  C\n",
    "∈ [0.1,10,1000,100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C :  0.1 \t train set accuracy :  0.95192 \ttest set accuracy :  0.9448 \n",
      "\n",
      "C :  10 \t train set accuracy :  0.96176 \ttest set accuracy :  0.93616 \n",
      "\n",
      "C :  1000 \t train set accuracy :  0.96248 \ttest set accuracy :  0.9262 \n",
      "\n",
      "C :  100000 \t train set accuracy :  0.9644 \ttest set accuracy :  0.92496 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in [0.1, 10, 1000, 100000 ]:\n",
    "    clf3 = svm.SVC(C=c)\n",
    "    clf3.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf3.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_pred)\n",
    "    \n",
    "    y_pred = clf3.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"C : \", c,'\\t', \"train set accuracy : \", train_acc,\"\\ttest set accuracy : \", test_acc,\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature_ABV_taste(d) for d in data]\n",
    "y = [d['beer/style'] == 'American IPA' for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:int(len(X)/2)], X[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y[:int(len(X)/2)], y[int(len(X)/2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta(X):\n",
    "    n_coeff = X.shape[1]\n",
    "    return np.random.randn(n_coeff)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def hypo_logReg(X, theta):\n",
    "    z = np.dot(X, theta)\n",
    "    return sigmoid(z)\n",
    "\n",
    "def costFun_logReg(X, y, theta):\n",
    "    m = X.shape[0]\n",
    "    h = hypo_logReg(X, theta)\n",
    "    \n",
    "    return (-1/m)*( np.sum( y * np.log(h) + (1 - y) * np.log (1 - h) ) )\n",
    "\n",
    "def cal_gradient_logReg(X, y, theta):\n",
    "    h = hypo_logReg(X, theta)\n",
    "    return np.dot( X.T, (h - y) ) / len(y)\n",
    "\n",
    "def perform_logReg(X, y, lr=0.01, num_itr=100000):\n",
    "    theta = init_theta(X)\n",
    "    costs = []\n",
    "    for i in range(num_itr):\n",
    "        gradient = cal_gradient_logReg(X, y, theta)\n",
    "        theta -= lr * gradient\n",
    "        if( i % 100 == 0):\n",
    "            costs.append(costFun_logReg(X, y, theta))\n",
    "    return theta, costs\n",
    "\n",
    "def predict_logReg(X, theta):\n",
    "    h = hypo_logReg(X, theta)\n",
    "    y_pred = (h >= 0.5).astype(np.int)\n",
    "    return y_pred\n",
    "\n",
    "def accuracy_logReg(y_true, y_pred):\n",
    "    correct_pred =[ true == pred for true, pred in zip(y_true, y_pred)] \n",
    "    accuracy =  np.sum(correct_pred) * 1.0 / len(correct_pred)\n",
    "    return accuracy\n",
    "\n",
    "def classificationReport(y_true, y_pred):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, costs = perform_logReg(X_train, y_train, lr=0.01, num_itr=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta             : [-1.73044082 -0.45385536  0.66272455]\n",
      "train set accuracy:  0.91356\n",
      "test set accuracy :  0.92188\n"
     ]
    }
   ],
   "source": [
    "print(\"theta             : {}\".format(theta) )\n",
    "\n",
    "y_pred = predict_logReg(X_train, theta)\n",
    "print(\"train set accuracy: \", accuracy_logReg(y_train, y_pred) )\n",
    "\n",
    "y_pred = predict_logReg(X_test, theta)\n",
    "print(\"test set accuracy : \", accuracy_logReg(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title('Cost reduction over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta              :  [-0.91205122] [[-0.91205122 -0.45266592  0.68382919]]\n",
      "train set accuracy : 0.91344\n",
      "test set accuracy  :  0.92188\n"
     ]
    }
   ],
   "source": [
    "print(\"theta              : \", clf.intercept_, clf.coef_)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "print(\"train set accuracy :\", accuracy_score(y_train, y_pred))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"test set accuracy  : \", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
